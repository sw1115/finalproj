---
title: "PPOL670 Final Project"
author: "Shiying Wang"
date: "5/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

## Which *Sustainable Development Goals (SDG)* are linked in the country's *National Determined Contributions (NDC)* document?  

This project aims to use supervised machine learning model to identify potential alignment between the targets, actions, policy measures and plans in countries' Nationally Determined Contributions (NDCs) and the targets of the Sustainable Development Goals (SDGs).  

### Keywords
Sustainable Development Goals, National Determined Contributions, Text Analysis, NLP, Classification

### Data Source
1. Nationally Determined Contributions submissions: [NDC Registry, UNFCCC.](https://www4.unfccc.int/sites/NDCStaging/Pages/All.aspx)
2. Sustainable Development Goals: [Sustainable Development Goals Knowledge Platform, United Nations](https://sustainabledevelopment.un.org/sdgs)
3. NDC-SDG linkages analysis data: [Climate Watch, World Resources Institute.](https://www.climatewatchdata.org/data-explorer/ndc-sdg-linkages?ndc-sdg-linkages-countries=All%20Selected&ndc-sdg-linkages-goals=All%20Selected&ndc-sdg-linkages-sectors=All%20Selected&ndc-sdg-linkages-targets=All%20Selected&page=1)

### Data Loading and Cleaning  
```{r install_package, include=FALSE}
# data pre-processing packages
library(tidyverse)
library(purrr)

# NLP processing pkgs
library(tm)
library(qdap)
library(quanteda)
library(tidytext)
library(topicmodels)
library(syuzhet)
library(igraph)

# data visualization packages
library(ggplot2)
library(wordcloud)
library(maps)
library(rJava)
```

```{r read_data}
linkages <- read.csv("linkages.csv", check.names = FALSE)
linkages <- linkages[ ,-10] # remove the extra column
glimpse(linkages)
```
```{r cleaning}
# generate unique id for each row
id <- rownames(linkages)
linkages <- cbind(id=id, linkages) 

# convert character to factor
linkages$id <- as.factor(linkages$id)

# clean the "goal" variable
levels(linkages$goal)

# There should be 17 SDGs but here we have 19 levels. So we need to clean the current factors list.
levels(linkages$goal)[levels(linkages$goal)=='Goal 1 - No Poverty '] <- 'Goal 1 - No Poverty'
levels(linkages$goal)[levels(linkages$goal)=='Goal 9 - Industry, Innovation and Infrastructure '] <- 'Goal 9 - Industry, Innovation and Infrastructure'
levels(linkages$goal)[levels(linkages$goal)=='Goal 11 - Sustainable Cities and Communities '] <- 'Goal 11 - Sustainable Cities and Communities'
levels(linkages$goal)[levels(linkages$goal)=='Goal 12 - Responsible Consumption and Production '] <- 'Goal 12 - Responsible Consumption and Production'

# change the order of different levels
linkages$goal <- factor(linkages$goal, levels = c("Goal 1 - No Poverty", "Goal 2 - Zero Hunger", 
                                                  "Goal 3 - Good Health and Well-being", "Goal 4 - Quality Education",
                                                  "Goal 5 - Gender Equality", "Goal 6 - Clean Water and Sanitation",
                                                  "Goal 7 - Affordable and Clean Energy", "Goal 8 - Decent Work and Economic Growth",
                                                  "Goal 9 - Industry, Innovation and Infrastructure",
                                                  "Goal 10 - Reduced Inequalities", "Goal 11 - Sustainable Cities and Communities",
                                                  "Goal 12 - Responsible Consumption and Production", "Goal 13 - Climate Action",
                                                  "Goal 14 - Life Below Water", "Goal 15 - Life on Land",
                                                  "Goal 16 - Peace, Justice and Strong Institutions",
                                                  "Goal 17 - Partnerships for the Goal"))


# check again
levels(linkages$goal)
```

In our current NDC-SDG linkages database, there are already 9602 observations coming from the analysis on over 190 countries' NDC submissions. For each row, we have the data for nine variables: **country** - the country name; **iso** - the three digit ISO code for that country; **ndc_text** - the specific text extracted from the NDC submission; **goal** - the tagged Sustainable Development Goal for the text from that country's NDC document; **target** - the more specific tagged target under that Sustainable Development Goal; **status** - whether the policy is an existing one or aims to be developed in the future; **sector** - which sector does the policy lie in; **climate_response** - whether it is an adaptation measure or a mitigation one; **type** - whether the text is identifying a specific action or just a needs & gaps to implement other actions.  

In our analysis, the most used variables will be **ndc_text**, **goal** and **target**.

### Exploratory Data Analysis  

#### Treemap: Count  Statistics for NDC-SDG Linkages  
First, we could use tree maps to visualize the count of sustainable development goals mentioned in countries' Nationally Determined Contribution submissions. We can tell from the tree map that, the most frequently mentioned sustainable development goals in countries' NDC submissions are **Goal 7** - Affordable and Clean Energy, **Goal 13** - Climate Action, **Goal 15** - Life on Land and **Goal 2** - Zero Hunger.  

More Specifically, we can further conclude that within each SDG, which targets are mentioned the most frequently. For example, under **Goal 7** - Affordable and Clean Energy, the most popular targets are **Target 7.2** - By 2030, increase substantially the share of renewable energy in the global energy mix, and **Target 7.3** - By 2030, double the global rate of improvement in energy efficiency. 

```{r treemap_1}
# library
library(treemap)

# Build Dataset
## only keep columns "goal" and "target" 
tree_linkages <- linkages[, c(4,5)] %>%
  group_by(goal) %>%
  count(target)

tree_linkages <- tree_linkages[-1, ]

# treemap
treemap(tree_linkages,
        index = c("goal",
                  "target"),
        vSize = "n",
        title = "Count Treemap for Sustainable Development Goals and Targets")

```


#### Wordcloud: Trending Words for Each Sustainable Development Goals  
Then we can do some preliminary text processing and create a word cloud to see the trending words for each sustainable development goals linkages.  

Here we use the most frequently tagged goal: **Goal 7** - Affordable and Clean Energy, as an example.  
```{r wordcloud_goal7}
goal7 <- linkages %>%
  filter(goal == "Goal 7 - Affordable and Clean Energy")

goal7_txt <- goal7$ndc_text

# remove special characters
goal7_txt_chrs <- gsub("[^A-Za-z]", " ", goal7_txt)

# convert to corpus
goal7_corpus <- goal7_txt_chrs %>%
  VectorSource() %>%
  Corpus()

head(goal7_corpus$content)

# convert to lower cases
goal7_corpus_lwr <- tm_map(goal7_corpus, tolower)

# remove stop words
goal7_corpus_stpwd <- tm_map(goal7_corpus_lwr, removeWords, stopwords("english"))

# remove extra spaces
goal7_corpus_final <- tm_map(goal7_corpus_stpwd, stripWhitespace)

# visualize the popular words (top 60 words based on frequency) (using qdap) ---- 
term_count <- freq_terms(goal7_corpus_final, 60)

# then we can create a vector of custom stop words based on the results
custom_stop <- c("energy", "use", "will", "mw", "pv", "sub", "including", "s",
                 "li", "based") # I exclude the word "energy" here to keep the count of words at the relatively same scale.

# remove custom stop words
goal7_corpus_refined <- tm_map(goal7_corpus_final, removeWords, custom_stop)

# creating a colorful word cloud (using RColorBrewer)
wordcloud(goal7_corpus_refined, max.words = 50,
          colors = brewer.pal(6, "Dark2"),
          scale = c(3, 0.5), 
          random.order = TRUE)
```

From the wordcloud we can tell that, energy efficiency, renewable energy, electricity and solar are the most popular words in the countries' NDC content tagged to SDG-7.  

### Tokenize the data
Then we may want to break the text into individual tokens which are simply individual words. Function **unnest_tokens** will be used here.  

First we can tidy the data by removing special characters.
```{r clean}
# only keep "id", "goal" and "ndc_text" columns.
new_linkages <- select(linkages, id, goal, ndc_text)

# convert to characters
new_linkages$id <- as.character(new_linkages$id)
new_linkages$ndc_text <- as.character(new_linkages$ndc_text)

# remove special characters
ndc_text <- new_linkages$ndc_text
ndc_text <- gsub("[^A-Za-z]", " ", ndc_text)

new_linkages$ndc_text <- ndc_text
```

#### Unigram
Then we can create unigram, which only has one word in each individual tokens. Extra spaces and stop words should also be removed here.
```{r token_unigram}
# break text into unigram
token1_linkages <- new_linkages %>%
  unnest_tokens(unigram, 
                ndc_text,
                to_lower = TRUE)
# check
head(token1_linkages, 10)

# remove the stop words
token1_linkages <- token1_linkages %>%
  filter(!unigram %in% stop_words$word)

head(token1_linkages, 10)

```

#### Bigram
Since tokenizing sentences to a unigram sometimes ignores the meaning of a word group collectively, we could further use a two word sequences which is "bigram".
```{r token_bigram}
token2_linkages <- new_linkages %>%
  unnest_tokens(bigram, 
                ndc_text,
                token = "ngrams",
                n = 2,
                to_lower = TRUE)

# check
head(token2_linkages, 10)

# remove the stop words
## seperate two words
token2_separated <- token2_linkages %>%
  separate(bigram, c("word1", "word2"), sep = " ")

token2_filtered <- token2_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

## recombine the words
token2_linkages <- token2_filtered %>%
  unite(bigram, word1, word2, sep = " ")

head(token2_linkages, 10)

```


### Remove sparse terms


### Document Term Matrix with TF-IDF weighting
Term frequency and inverse document frequency (TF-IDF) can help us decrease the weight of the common words, which are words that appear in all 17 goals. For those common words, both idf and thus tf-idf will be zero.  


```{r tfidf1}
# calculate the number of times that the unigram is used in that goal
unigram_tfidf <- token1_linkages %>%
  count(goal, unigram, sort = TRUE)

# calculate tfidf
unigram_tfidf <- unigram_tfidf %>%
  bind_tf_idf(unigram, goal, n) %>%
  arrange(desc(tf_idf))

unigram_tfidf

# visualize the result
unigram_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(unigram = factor(unigram, levels = rev(unique(unigram)))) %>% 
  group_by(goal) %>% 
  top_n(5) %>% 
  ungroup() %>%
  ggplot(aes(unigram, tf_idf, fill = goal)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~goal, ncol = 4, scales = "free") +
  coord_flip()
```


```{r tfidf2}
# calculate the number of times that the bigram is used in that goal
bigram_tfidf <- token2_linkages %>%
  count(goal, bigram, sort = TRUE) %>%
  bind_tf_idf(bigram, goal, n) %>%
  arrange(desc(tf_idf))

bigram_tfidf

# visualize the result
bigram_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(goal) %>% 
  top_n(5) %>%
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = goal)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~goal, ncol = 4, scales = "free") +
  coord_flip()

```


### Classification Modeling (bigram)
#### knn
#### Neural Network
#### Random Forest

### Reference

